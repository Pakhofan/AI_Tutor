Janet: Good evening, Prof Chang, I am Sam from IT support center. How can I help you? Prof. Chang: Good evening Sam, good to hear that you are here at this hour. My PC just hung up and no response. Please help. Janet: I see. Maybe you can switch off your PC and power it on again after 1 minute. See whether it works. Prof. Chang: Okay, I do it now. Janet: Is it okay now? Prof. Chang: It came up with a blue screen and have several choices to press different functions. What should I do? Janet: Good, it is the emergency system support screen. Please press F4 to continue the system bootup sequence and we can remote control your PC to fix for good. Prof. Chang: Okay. I just pressed the F4 button. PC is now bootup and start remote connection to IT support center. Janet: Yes. I can see it now. The problem is not too serious. Please give me 5 minutes to fix it. Prof. Chang, you PC is okay now. Please have a look. Prof. Chang: Excellent work! Everything is okay now. So many thanks, Sam. Janet: You are welcome. Have a nice evening. Prof. Chang: You too, bye! Abstract This chapter begins with the introduction of human language and intelligence. We also introduce the six linguistics levels in human language. Next, we study NLP main components that include natural language understanding, speech recognition, syntactic analysis, semantic analysis, pragmatic analysis, and speech synthesis followed by major NLP applications which include machine translation, information extraction, information retrieval, sentiment analysis, question and answering chatbots. Is Janet a human system support? Maybe. Can she be a service robot? The answer is definitely yes . At the time the author wrote the book on intelligent agent over a decade ago, Natural Language Processing technology was only a research topic. But today, NLP for customer service become so popular in major corporations that we won't even know the customer service agent is a robot or a human. How can we do it? In this chapter, we will introduce this fascinating technology by studying the main components of NLP. We will soon find out that NLP technology is closely related to different disciplines that include linguistics, statistical engineering, machine learning, data mining, human voice processing, etc. We will also be surprised by AI scientists' and NLP engineers' genius and efforts during the past two decades to turn this important research topic into commercial products that can be a great help in many applications to our daily activities. Let us begin our journey on natural language processing with human language and intelligence . How we behave define who we are . The author thinks the clause is candid. Since we cannot know what people are thinking about, the only cue to evaluate or form the opinion of a person whether is good or bad, genius or dumb can only be achieved by observing their behaviors. The most direct way is to observe what he/she said by simple dialogues. This was why Sir Alan Turing; the father of AI devised the famous Turing Test in the 1950s as a way to judge whether a machine has intelligence (Fig. 6.1). In terms of AI perspective, the core technology of Turing Test is in fact NLP, the technology to recognize and understand human language questions, and how to respond also in human language to the judge. It is the ultimate challenge of NLP. Human language is a crucial component in civilization and one of the most fundamental aspects of our behaviors. In a general sense, it can be categorized into two main aspects: written and oral languages. For written language, the main function is to store and pass our knowledge to others of this and future generations. For oral language, the core function is to act as a medium for communication behind a person to another in our daily activities. Language study is exceedingly important that different disciplines have their own focuses and interpretation. Each discipline comes with its own set of language-related problems to tackle and a set of solutions to address those problems. Table 6.1 shows a summary of these language disciplines and solutions to related problems. Levels of linguistic refer to the functional analysis of any human language that include both written and spoken languages. In terms of linguistic analysis, there are six levels of linguistics classified into three main categories. The two basic linguistic levels of sound include: phonetics and phonology sound levels. These two levels related to the sounds of spoken language. Phonetics is about the physical aspect of sounds; it studies the production and perception of sounds called phones . Phonetics deals with the production of speech sounds by human, often without prior knowledge of the spoken language. Phonology is about the abstract aspect of sounds and studies phonemes . Phonology is about establishing what are the phonemes of a given language, i.e. those sounds can deliver a different meaning between two words. For instance, the vowels in the English words cool , whose, and moon are all similar but slightly different. The different variants depend on the different contexts in which they occur. Intermediate linguistic levels of structure refer to the basic language which include two levels: morphology and syntax levels of language structure. Morphology is the level of forms and words . It normally comprehends by grammar. The term morphology refers to minimal forms in language analysis, comprised of sounds to construct words that have either a grammatical or a lexical function. Lexicology is concerned with the study of the lexicon from a formal point of view and is thus linked closely to morphology . Syntax is the level of clauses and sentences. It is concerned with words meanings in combination with each other to form phrases or sentences. It involves particularly different meaning by changes in word order, addition or subtraction from sentences or changes in sentence forms. Furthermore, it deals with the relatedness of different sentence types and ambiguous sentences analysis. Advanced linguistic levels of meaning refer to the actual meaning of the language, which include two levels: semantic and pragmatic levels of language meaning. Semantics is the meaning area. It is thought that semantics is covered by areas of morphology and syntax. Thus, for every language level there exists lexical, grammatical, sentence, and utterance meaning. Pragmatics is the use of language in specific situations. The meaning of sentences does not have to be identical to an abstract form in practical use. For utterance meaning, the area of pragmatics relies strongly for its analyses on the notion of speech act which concerned with actual language performance. This involves the notion of proposition––about the content of a sentence with its intent and effect of an utterance (Fig. 6.2).Ambiguity in Human Language Ambiguity and uncertainty in language ambiguity used in natural language processing refer to the ability of being understood in more than a single way. Natural language is ambiguous inducing NLP with subsequent types of ambiguities. Lexical Ambiguity The ambiguity of a single word is called lexical ambiguity. For example, consider the word silver as a noun of metal, an adjective of silver colored, or a verb of process of silvering. Syntactic Ambiguity This kind of ambiguity occurs when a sentence is parsed in different ways. For example, the sentence: The man saw a girl with telescope . It is ambiguous whether the man saw the girl carrying a telescope or he saw her through his telescope. Semantic Ambiguity This kind of ambiguity occurs when the meaning of words can be misinterpreted. In other words, semantic ambiguity occurred when a sentence contains an ambiguous word or phrase. For example, the sentence: The car hit the dog while it was moving is having semantic ambiguity because the interpretations can be: The car, while moving, hit the dog and the car hit the dog while the dog was moving . Pragmatic Ambiguity Pragmatic ambiguity refers to a situation where the context of a phrase gives multiple interpretations. In simple words, we can say that pragmatic ambiguity arises when the statement is not specific. For example, Marie says: I'll go to river bank this morning . The meaning of river bank normally refers to probably the bank of the river. But if she says: I'll go to riverbank this morning to take some cash. In that case, the second part of the clause to take some cash gives us more clue about what she refers to riverbank maybe is the name of a bank. Figure 6.3 shows a typical example of ambiguity in a language in high level of pragmatic meaning. First Stage—Machine Translation on NLP The history of NLP can be traced back to the seventeenth century when philosophers such as polymath Gottfried Wilhelm Leibniz and philosopher, mathematician, and scientist René Descartes proposed to use codes to relate words between different languages. Although the proposals remained only theoretical then, they laid the ground for language translation machine development. The first invention patent related to translation machine was applied by inventor and engineer Georges Artsrouni proposal in 1933. However, the history of NLP officially began in the 1950s with Sir Alan Turing for his famous article publication Computing Machinery and Intelligence and the proposal of Turing Test to explore machine intelligence using NLP as judging criteria. At the time, NLP was mainly focused on intelligent machine R&D for language translation––machine translation. The first international conference on machine translation was held in 1952 and the second one in 1956. NLP remained focused only on machine translation were used mainly on simple rule-based methods and statistical techniques. In 1954, Georgetown-IBM experiment involved fully automatic translation of more than 60 Russian sentences into English. The inventors at that time were overoptimistic to claim that the entire machine translation problem would be completed solved within 3–5 years. However, real progress took longer than expected. It was not until the Noam Chomsky's Syntactic Structures helped to revolutionize linguistics with universal grammar in 1957. The work presented in Teddington International Conference on Machine Translation of Languages and Applied Language Analysis reached its climax in 1961. After the release of the ALPAC report in 1966 revealed that the 10-year-long research had failed to fulfill its original expectation of machine translation in result of funding for all related research and projects were reduced substantially. Second Stage—Early AI on NLP. When AI became popular at this period of time, major NLP development was focused on how AI can be applied to knowledge exploration, so-called ontology, and its role on construction and manipulation of meaning representations. A typical example includes BASEBALL system development in the late 1960s, a question-answering AI-based expert system. However, the input to this system was restricted and involved simple language processing. A more advanced NLP system was proposed by cognitive scientist Prof. Marvin Minsky in 1968––a major founder of AI. Compared with BASEBALL Q&A system, this NLP system employed AI-based inference engine and knowledge base for Q&As interpretation. Augmented Transition Network was introduced to represent natural language input by distinguished software engineer Prof. William A. Woods in 1970. During that time, many programmers also began to write conceptual ontologies which structured real-world information into computerunderstandable data. But because of these high expectations on AI and expert systems were not truly realized, both US Government and commercial sectors withdraw further research funding led to the first winter of AI. Third Stage—Grammatico-Logical on NLP This phase can be described as the grammatico-logical phase . Due to the failure of building practical system at the last phase, researchers moved toward the use of logic for knowledge representation and reasoning in AI. The grammatico-logical approach provided succor and powerful generalpurpose sentence processors like SRI's core language engine and discourse representation theory , which offered a means of tackling more extended discourse toward the end of decade. In this phase, practical resources and tools like parsers, e.g. Alvey natural language tools along with more operational and commercial systems for database query. Although NLP R&D were bounded by the computational capacity of computer systems in this period, the work on lexicon provided a solid foundation and direction of grammatico-logical approach for future development. Fourth Stage—AI and Machine Learning Most natural language processing systems were based on complex sets of hand-written rules until the 1980s. The rebirth of AI coined by physicist and scientist Emeritus Prof. John Hopfield success on his ground-breaking Hopfield Network in machine learning was revolutionary to introduce machine learning algorithms for language processing. Thanks for the improvement on computer technology in terms of computational capacity and memory storage together with the dominance of Chomskyan theories of linguistics, whose theoretical underpinnings discouraged the sort of corpus linguistics machine learning approach to language processing. This stage was also known as NLP lexical & corpus . It had a lexicalized approach to grammar appeared in the late 1980s and became influential. Watson, a question-answering computer system capable of answering questions posed in natural language was developed by a research team of IBM DeepQA project led by scientist Dr. David Ferrucci in 2006. Fifth Stage—AI, Big Data, and Deep Networks This period considers as years of AI, Big Data, and Deep Networks . With the advance of cloud computing technology, massive knowledge representation, data mining, and knowledge discovery favor AI-based NLP development, particularly in agent ontology R&D is the core knowledge center in every NLP system. Representation learning uses agent ontology and deep neural network style machine learning methods became widespread in natural language processing in the 2010s. For example, neural machine translation is a new type of deep network-based NLP system emphasizes on deep learning based approaches to machine translation directly learn sequence-to-sequence transformations, instead of using traditional statistical machine translation. The technological maturity of voice recognition and human voice synthesis also favors NLP-based AI applications development such as AI chatbots, customer service robots, etc. What is Natural Language Processing? NLP can be defined as human language automatic processing. In some sense, the term NLP is sometimes narrowly used often excluding information retrieval and even machine translation. Many computer scientists consider NLP as computational linguistics . It is rather true in terms of computer science, NLP can be considered as a kind of computer modeling or computerization of linguistics, alike the term computational finance refers to computational modeling of finance theory. NLP is a multidiscipline topic that involves extensive knowledge and basic concepts on linguistics and logic theory in theoretical mathematics. At present, NLP research covers cognitive science, psychology, and even philosophy in terms of epistemology and ontology. NLP and AI NLP is a field of AI in which computers analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, AI developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation. NLP is used to analyze text, allow machines to understand how human speak and respond. This human–computer interaction enables real-world applications like automatic text summarization, sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. NLP is frequently used for text mining, machine translation, and automated question-answering. With the rapid growth of AI and computer technology, current NLP research and implementation also involve AI-based machine learning, data mining, deep learning, and agent ontology (Fig. 6.4). NLP consist of three main components. Natural Language Understanding corresponds to all basic functions and operations of NLP from human language voice recognition to three levels analysis of understanding the meaning of spoken language: syntax, semantic, and pragmatic analysis . Traditional NLP with a restricted domain of application is the only focus of this component. Knowledge Acquisition and Inferencing Once the spoken language is clearly understood from NLU, KAI focuses on proper response and answer generation. In terms of machine learning and AI, it is a kind of knowledge acquisition and inferencing problem. Traditionally, such a task is achieved by the rule-based system. That is an if–then kind of question and response which is frequently used in many expert systems. However, with the complexity of natural language and conversation, most rule-based systems failed to apply successfully. To solve this intrinsic problem, most KAI system will attempt to restrict the knowledge domain to certain area such as customer service knowledge for specific industry, e.g. insurance, IT. With the advancements in AI technology, a new technology on agent ontology has been implemented with certain success. We will study in detail about the Ontological-based Search Engine in the next chapter. Natural Language Generation involves reply, respond, and feedback generation in the human– machine conversation. This consists of the process of response formulation into texts and sentences with the target language, text-to-voice synthesis based on the target language to produce near-human voice response (Fig. 6.5). Natural Language Understanding focuses on the recognition and understanding of the spoken language. It consists of four main processes: speech recognition, syntax analysis, semantic analysis and pragmatic analysis. Figure 6.6 shows the systematic diagram of NLU. Speech Recognition It is the NLP first phase, which corresponds to phonetics implementation, phonology, and morphological processing of the spoken language mentioned in the linguistic model. The main purpose of this phase is to break chunks of spoken language input into sets of tokens, which correspond to paragraphs, sentences, and words. Current speech recognition applies frequency spectrogram technology to extract different frequencies of spoken sounds for speech recognition. For example, a word like uncertain can be broken into two sub-word tokens as un-certain. Syntax Analysis It is the NLP second phase, which corresponds directly to the first level for the structural meaning analysis of spoken sentence(s). The purpose of this phase is twofold: (1) to check that a sentence is well formed and (2) to break the spoken sentence(s) into a syntactic structure that can reflect the syntactic relationships between different words. For example, the sentence: The apple goes to the girl would be rejected by syntax analyzer or parser. Semantic Analysis It is the NLP third phase which corresponds directly to the second level for semantic meaning analysis of the spoken sentence(s). The purpose of this phase is to extract the exact meaning, or one can say the meaning defined by the dictionary extracted from the text. In order words, the extracted text is checked for its meaningfulness. For example, the semantic analyzer would reject a sentence like hot snowflakes . Pragmatic Analysis It is the NLP fourth phase and the most difficult level of meaning analysis of the spoken sentence(s). Pragmatic analysis deals with outside world knowledge , which means knowledge is external to the spoken sentence(s). Pragmatics analysis focuses on what was described is reinterpreted by what it actually meant, deriving various language aspects from real-world knowledge. For example, the sentence: Will you crack the door? Is getting hot. Semantically, the word crack would mean to break, but pragmatically we know that the speaker means to slightly open the door to let in some air. Speech recognition (Li et al. 2015)––technically known as the voice-to-text process is a complex process which consists of four basics steps: (1) Voice capture/recording ; (2) Analog-to-Digital Conversion; (3) Frequency spectrogram generation, and (4) Phonemes/words generation as shown in Fig. 6.7. In voice capture/recording step, spoken voice is captured or recorded voice communication devices such as PC's microphones or the user's mobile phone, via internet or mobile network to NLP system such as technical support of IT computer. In the Analog-to-Digital Converter step, the system translates these analog sound wave into digital data so that it can be processed by the speech recognition system. To achieve this, the system samples or digitizes the sound wave by taking measurements of sound waves at different time intervals. In the frequency spectrogram generation step, the system filters background noises to improve sound quality, and then separate the sound waves into different bands of frequency to generate the so-called Frequency Spectrogram. It also normalizes the sound or adjusts it to a constant volume level. As human do not always speak at the same speed, so the sound must be adjusted to match with the speed of template sound samples already stored in the system's memory. In phonemes/words tokenizatio n step, the processed signal is divided into small segments as short as a few hundredths of a second, or even thousandths in the case of plosive consonant sounds––consonant stops produced by obstructing airflow in the vocal tract––like p or t . The system then matches these segments to known phonemes with the appropriate language. A phoneme is the smallest element of a language––a representation of sounds we make and put together to form meaningful expressions. There are approximately 40 phonemes in the English language, while other languages have more or fewer phonemes. The most widely commercially used voice-to-text tokenization technology is based on hidden Markov model, deep neural networks are also used nowadays. Next, let us take a look at how HMM works. Hidden Markov model is a powerful statistical model based on the Markov process concept that outputs a sequence of symbols or quantities. HMM is used for the voice signal tokenization because it can be regarded as a piecewise stationary signal. Human voice can basically be modeled as a stationary process within a short timescale such as 5–10 ms. Speech can be thought of a Markov model for many stochastic purposes. Another reason why HMM is popular because it is: (1) reliable; (2) easy to implement technically; (3) the Markov chains can be trained automatically and computationally feasible to use. HMM is a system where a variable can switch between several states, generating one of several possible output symbols with each switch. The sets of possible states and unique symbols may be large, but finite and known. The basic process in HMM in speech recognition are. (1) Inference: given a specific sequence of output symbols, compute the probabilities of one or more candidate state-switch sequences. (2) Pattern matching : find the state-switch sequence most likely to have generated a specific output-symbol sequence. (3) Training : given examples of output-symbol sequence data, compute the state-switch/output probabilities that fit this data best (Fig. 6.8). Figure 6.8 shows a typical HMM model of spoken voice tokenization at phonemes and words level. As shown, a comprehensive lexicon for the targeted language is required to accomplish the speech recognition process. We will study about it in the coming section. Syntax analysis is also known as parsing . Parsing is the process of determining whether a string of tokens can be generated by the grammar. It is performed by syntax analyzer which can also be termed as a parser. Figure 6.9 shows the basic parsing mechanism in syntactic analysis. As shown, tokenized input texts generated by the speech recognition system are fed into the lexical analyzer and cross-checked with the lexicon database such as WordNet to examine for correct syntax and grammar. After that, it passes into the parser to establish a data structure generally in the form of a parse tree or other syntax structures. The main roles of the parse include to. check for any syntax or grammatic error, recover from commonly occurring error so that the processing of the construct and modify the parse tree, remaining program can be continued, construct and modify the symbol table, and produce intermediate representations for information retrieval. It may be defined as a software component designed for taking input data and giving input structural representation after checking for correct syntax as per formal grammar. It also builds a general data structure in the form of a parse tree or abstract syntax tree or other hierarchical structure. Grammar is essential and important to describe the syntactic structure of well-formed languages. In the literary sense, it defines conversation syntactical rules for different languages. Linguistics have attempted to define grammar since the inception of natural languages like English, Chinese, Japanese, Hindi, etc. The theory of formal languages is also applicable in the fields of computer science mainly in programming languages and data structure. Even for a computer language such as Java, the precise grammar rules state how functions are made from lists and statements. What is a Parse Tree? Parse Tree is defined as the graphical depiction of a derivation. The start symbol of derivation serves as the root of the parse tree––the sentence root node. The other nodes include all constitutional components in a sentence that include: Noun_Phrase, Verb_Phrase, Determiner, Verb, Noun, Proper_Noun, Adjective, Adverb, etc. For each parse tree, the leaf nodes are terminals and interior nodes are nonterminals. A property of parse tree is that in-order traversal will produce the original input text sentence. So, the main function of parsing is: Given a sentence with a grammar, the parser will check the sentence whether it is correct according to grammar and if so, returns a parse tree representing the sentence structure. Figure 6.10 shows an example of parse tree for the sentence: This diagram is illustrating the parsing tree What is Semantic Analysis? Semantic analysis is to extract the actual meaning of the text or may say the dictionary meaning . Anyway, the main task of semantic analysis is to examine the meaning fulness of the text. One may wonder: Lexical analysis in syntactic analysis in some sense already examined the meanings of words, so what is the difference between lexical and semantic analysis? The truth is: Lexical analysis only looks into the meaning of individual words from lexicon, while semantic analysis extracts the overall meaning of the text that most likely involves the combination of more than one or several words token in order to extract the actual meaning of the complete text message. For example, the sentence: Einstein is great scientist means Albert Einstein, the one who proposed General Relativity is a great scientist ; or another scientist called Einstein is a great scientist? To solve this problem, we need to know more about the sentence to provide more cues which Einstein refers to, in order to investigate the actual meaning of the sentence. That is the reason why we need semantic analysis. Semantic Network A semantic network is a knowledgebase that represents semantic relations between concepts in a network. This is often used as a form of knowledge representation. In short, a semantic network is a directed or undirected graph consisting of vertices, which represent concepts, and edges that represent semantic relations between concepts, mapping, or connecting semantic fields. In general, most semantic networks are cognitively based. They also consist of arcs and nodes, which can be organized into a taxonomic hierarchy. Figure 6.11 shows a typical concept example of a semantic network for the word mammal . As shown, we can extract several useful knowledges from a simple semantic network. To use the semantic network, simply start with the node corresponds to the word we interest, forward an arrow of the graph to find a path. Each path corresponds to a specific knowledge related to the concept we interest. For instance: Mammal is an animal. Bear is a mammal with vertebra. Whale is a mammal, lives in water. Cat is a mammal has fur. What is Pragmatic Analysis? Pragmatic analysis is the last phase of linguistic analysis. It is part of the process of extracting information from text. Specifically, it is the portion that focuses on taking a text structure set and reasoning out the actual meaning. Unlike semantics which examines meaning that is conventional or coded in a given language, pragmatic analysis studies how the transmission of meaning depends not only on structural and linguistic knowledge of the speaker but also on the context of the utterance, that might involve any pre-existing knowledge and the inferred intent of the speaker. The most famous example is the clause: Raining cats and dogs . We all know it means raining heavily. But how can we draw this knowledge. As such concept of knowledge is totally unrelated to the syntax, semantic, or even the semantic networks of either: cat, dog, or rain––this is the task for pragmatic analysis . In other words, the main purpose of pragmatic analysis is trying to extract the true knowledge of spoken speech, which may be or may not be directly reflected by its semantic meaning. Due to high complexity and ambiguity to extract the embedded meanin g of spoken language, not only related to the spoken message, but also other related knowledge and concepts outside the topic context and knowledge domain. Pragmatic analysis is believed to be one of the most difficult topics in linguistic and AI in terms of NLP implementation with knowledgebase and search engine. It is different from voice recognition; syntactic and semantic analysis are technically mature enough as widely adopted methods and technology. Pragmatic analysis is still in the R&D stage without any dominant technology and solution. Latest research of pragmatic analysis includes R&D of an emerging AI technology––Agent Ontology, which focuses on the ultimate problem of how human knowledge is generated, stored, and retrieval. More importantly, different concepts and ideas are related together to retrieve actual and embedded meaning, which will be studied in the next chapter––Ontologicalbased Search Engine . Figure 6.12 shows a snapshot of an example ontology graph. Text-To-Speech Synthesis Speech synthesis is an artificial simulation of human speech by a computer or other device mostly used for translating text information into audio information as a counterpart of voice recognition. It is also known as TTS technology. Further, speech synthesis is an assistive technology used by vision-impaired individuals to read text contents. It is alike speech recognition as a mature technology widely used in many related applications such as voice-enabled services and mobile applications of our daily activities. Speech Synthesis System A typical speech synthesis system consists of three main modules: text analyzer; linguistic analyzer, and waveform generator as shown in Fig. 6.13. The main function of the text analyzer is to convert response text generated, says, from ontology-based knowledgebase into words tokens––tokenization process . After that, it passes to the linguistic analyzer for further processing. In the linguistic analyzer , it assigns phonetic transcriptions to each word, divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information jointly make up the symbolic linguistic representation. The processed phonemes then pass to the waveform generator . The waveform generator , commonly known as speech synthesizer converts the symbolic linguistic representation into sound. In most systems, this part includes the computation of target prosody, which is then imposed on output speech in the form of human voices. Corpus In linguistics, corpus is a large and structured set of machine-readable texts produced in a natural communicative setting. Technically speaking, a corpus can be derived in different ways like text that was originally electronic transcripts of spoken language, optical character recognition, etc. Language is infinite but a corpus must be finite in size, we need to sample and proportionally include a wide range of text types to ensure a good corpus design. Another important element of corpus design is its size. How large a corpus should be? There is no specific answer to this question. Corpus size depends upon the purpose of intension with other practical considerations as follows: Kind of query anticipated from users. The methodology used by users to study the data. Availability of the source of data. Corpus size also increases with technology advancement. For example, the size of a Brown and LOB corpus is around 1 million words, while the frequently used Bank of English corpus today is over 650 million words. TreeBank and ProBank Corpus TreeBank corpus is linguistically parsed text corpus that annotates syntactic or semantic sentence structure. Specialist in English language and linguistics Emeritus Prof. Geoffrey Leech coined the term treebank as the most common way of representing grammatical analysis by means of a tree structure. Semantic and syntactic treebanks are the two most common types of treebanks in linguistics. ProBank Corpu s also known as proposition bank annotated with verbal propositions and their arguments was developed by computer scientist Prof. Martha Palmer et al. The corpus is a verb-oriented resource, annotations here are more closely related to the syntactic level. In NLP, PropBank project has played a significant role to assist in semantic role labeling. Figure 6.14 shows a sample snapshot of the parse tree construction using TreeBank Corpus. WordNet and VerbNet WordNet and VerbNet are the two most frequently used corpus in NLP. VerbNet is the hierarchical domain-independent and largest lexical resource present in English that incorporates both semantic as well as syntactic information about its contents. VN is a broad-coverage verb lexicon having mappings to other lexical resources such as WordNet, Xtag, and FrameNet . It is organized into verb classes by refinement and addition of subclasses to achieve syntactic and semantic coherence among class members. WordNet created by Princeton University is a lexical database for the English language. WordNet is a lexical database of semantic relations between words in more than 200 languages and the most frequently used corpus in the world, both for commercial and academic research in NLP, knowledgebase, and agent ontology R&D. In WordNet, nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms called synsets . All synsets are linked with the assistance of conceptual–semantic and lexical relations. In terms of information systems, WordNet is used for various purposes like word-sense disambiguation, information retrieval, automatic text classification, and machine translation. One of the most important uses of WordNet is to find out the similarity among words. Due to its popularity, various functions and algorithms on syntactic and semantic analysis have been implemented in various NLP-related development platforms as functional libraries or packages that include C, C++ , Perl, ADW in Java, and NLTK in Python. Figure 6.15 shows a snapshot WordNet database from the perspective of adjective “good” . After over 20 years of R&D and actual implementation, NLP technology is now being used in many applications that are closely related to our daily activities. They include machine translation, information extraction, information retrieval, sentiment analysis , question & answering robots, etc. as shown in Fig. 6.16. Machine Translation Machine translation is the earliest, most well-studied and one of the most important NLP applications. A major challenge in MT nowadays is twofold: (1) the naturalness--machine translation that is natural in the target language while preserving the exact meaning expressed by input; (2) the adequacy ––the degree of MT to which output reflects the meaning of the source. These two are often in conflict, especially when the source and target languages are not similar. Experienced human translators address this trade-off in an artistic way. The goal of nowadays MT is to apply various AI technologies such as deep learning to learn from experts to achieve human-quality translations. Information Extraction is the task of extracting structured information automatically from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases, this activity concerns processing human language texts by means of NLP. Recent activities in multimedia document processing like automatic annotation and content extraction out of images, audio, video, and text documents could be regarded as information extraction. Due to the difficulty of the problem, many commercial IE applications are domain specific such as the focus of a specific discipline or topic interest. Information Retrieval is a software program that deals with the organization, storage, retrieval, and evaluation of information from document repositories particularly textual information. The system assists users in finding the information they require but does not explicitly return answers to the questions. It informs the existence and location of documents that might consist of the required information. The documents that satisfy the user's requirement are called relevant documents. A user who needs information will have to formulate a request in the query form using NLP. Then the IR system will respond by retrieving the relevant output, in documents form about the required information. In fact, the main objective of the IR system is to develop a model to retrieve information from the repositories of documents. A typical example of an IR system is a so-called ad hocretrieval problem. Figure 6.17 shows the process flowchart of a typical information retrieval system using ad hoc retrieval method. In ad-hoc retrieval, the user must enter a query in natural language that describes the required information. Then the IR system will return the required documents related to the desired information. For example, suppose we search something from the Internet, and it gives some exact pages that are relevant per our requirements but there can contain some nonrelevant pages. This is due to the ad-hoc retrieval problem. Sentiment Analysis Sentiment analysis is a type of data mining that measures the inclination of people's opinions through NLP, which are used to extract and analyze subjective information from the Web, usually social media, and similar sources. The analyzed data quantifies the general public's sentiments or reactions toward certain products, people, or ideas and reveal the contextual polarity of the information. Customer services use sentiment analysis, an NLP application to identify user's opinion and sentiment. It will enable companies to understand what their customers think about the products and services. Companies can also judge their overall reputation from customer posts with the assistance of sentiment analysis. In this way, we can say that beyond determining simple polarity, sentiment analysis understands sentiments in context allowing us to better understand what is behind the expressed opinion. Figure 6.18 shows a typical scenario of sentiment analysis using NLP technology. As shown, with the integration of data mining technology and NLP, user's responses and comments on various topic of interest can be converted into machine-understandable concepts and ideas that can be classified effectively into different degrees of emotion and response, that can be used by companies to better understand and analyze customers' needs. For news agency and public forum, sentiment analysis together with NLP technology can be used to data-mine public opinions and comments more effectively and objectively. In fact, NLP-based sentiment analysis is widely used to major social media and forum to users' opinion and real-time responses to some ad-hoc events and incidences. Question and Answering Robots Another main application of natural language processing is question answering robots, or so-called chatbot. In general, Q&A systems is the ultimate challenge of NLP and AI which is the main theme of the Turing Test. It concerns with the building of the AI system and automatically answers questions raised by a human using our own languages. In other words, the Q&A not only need to recognize and understand human language, but also need to know the actual meaning from syntax, semantic up to pragmatic levels. It also needs to response with a human voice, involves high-level knowledge-based and inferencing, together with human voice generation system. With the rapid growth in AI and NLP technology, Q&A robots and systems are widely used in many industries that include Technical support robots, e.g. provide IT basic technical support via Customer service robots, e.g. promote products and after-sale support internet and traditional hotlines. Language learning tutor, e.g. teach and train students in language centers. services. Figure 6.19 shows a typical scenario of Q&A customer services robots. Companion robot is not a new thing in AI world. It dates back to the 1980s, where Japanese industries had already developed several famous companion robots to interact with human and provide limited NLP capability. With the advanced AI, computing, and robotic technology, nowadays companion robots are capable to provide dynamic services, such as foreign language learning robots to teach students how to speak foreign languages, i.e. English LLR teach Asian students to learn English, or Spanish LLR teach English-language-oriented students to learn Spanish in their daily activities. Suppose you are an English LLR system designer. Based on the NLP technology learnt in this chapter, together with various machine learning and data mining techniques learnt from previous chapters: (1) What are the three basic machine learning techniques in AI? (2) Discuss and explain how these machine learning techniques can be applied to English learning? (3) Discuss and explain how to integrate these machine learning techniques, together with the NLP technology to implement an English Learning Robot? (4) Many learning systems have different levels of challenges. Suppose you need to design this English LLR with 3 levels of challenges. What are these 3 levels of challenges? (5) What kinds of AI and NLP methods do you use to implement these 3 levels of function? (Fig. 6.20) In this chapter, we discuss an exceedingly challenging and important AI technology––natural language processing. NLP is not a new topic. In fact, the Turing Test proposed by Sir Alan Turing in the 1950s was the focus on NLP performance of the machine to determine the degree of intelligence. It was also one of the core components of Generalized AI and robot design during the 1960–80s. However, owing to AI over-expectation and computational capability limitation, NLP technology development was sluggish and mainly focused on statistical-based machine learning applications. With the advancements in computational speed and AI popularity, machine learning, deep network, big data and data mining, NLP technology and related applications had evolved rapidly in the past 20 years. Today, various NLP-related technology such as human voice synthesis systems for car navigation, information retrieval, information extraction, NLP-based customer services robots, sentiment analysis in social media, machine translation apps, and systems with Q&A chatbots became part of our daily activities. NLP is in fact not only a human voice related technology, but also closely related to how we acquire, learn, store, and manipulate our knowledge. The so-called ultimate challenge of AI–knowledgebase and agent ontology problem. In the next chapter, we will study this challenging topic––Ontologicalbased Search Engine(Fig. 6.21).